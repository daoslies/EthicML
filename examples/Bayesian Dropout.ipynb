{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, Reduction\n",
    "\n",
    "import ethicml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlwaysDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs, training=None):\n",
    "        # always set training to 'True'\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = 0.9\n",
    "length_scale = 1.0\n",
    "precision = 100.0\n",
    "batch_size = 32\n",
    "hidden_units = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = (keep_prob * length_scale**2) / (2 * batch_size * precision)\n",
    "NN_drop = tf.keras.Sequential([\n",
    "    AlwaysDropout(1 - keep_prob),\n",
    "    Dense(hidden_units, activation=\"tanh\", kernel_regularizer=l2(weight_decay),\n",
    "          use_bias=True, bias_regularizer=l2(weight_decay)),\n",
    "    AlwaysDropout(1 - keep_prob),\n",
    "    Dense(1, activation=None, kernel_regularizer=l2(weight_decay),\n",
    "          use_bias=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = tf.keras.Sequential([\n",
    "    Dropout(1 - keep_prob),\n",
    "    Dense(hidden_units, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "          use_bias=True, bias_regularizer=tf.keras.regularizers.l2(weight_decay)),\n",
    "    Dropout(1 - keep_prob),\n",
    "    Dense(1, activation=None, kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "          use_bias=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, s, y = ethicml.data.load.load_data(ethicml.data.Adult())\n",
    "x, s, y = x.to_numpy(), s.to_numpy(), y.to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "x = stats.zscore(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).cache().shuffle(\n",
    "    len(x), seed=888, reshuffle_each_iteration=False)\n",
    "test_ds = dataset.take(len(x) // 5).batch(len(x) // 5)\n",
    "train_ds = dataset.skip(len(x) // 5).shuffle(len(x) // 5, seed=888).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1221/1221 [==============================] - 3s 3ms/step - loss: 0.3511 - accuracy: 0.8396 - val_loss: 0.3427 - val_accuracy: 0.8389\n",
      "Epoch 2/10\n",
      "1221/1221 [==============================] - 2s 2ms/step - loss: 0.3485 - accuracy: 0.8400 - val_loss: 0.3415 - val_accuracy: 0.8399\n",
      "Epoch 3/10\n",
      "1221/1221 [==============================] - 2s 2ms/step - loss: 0.3482 - accuracy: 0.8391 - val_loss: 0.3390 - val_accuracy: 0.8392\n",
      "Epoch 4/10\n",
      "1221/1221 [==============================] - 2s 2ms/step - loss: 0.3481 - accuracy: 0.8400 - val_loss: 0.3390 - val_accuracy: 0.8417\n",
      "Epoch 5/10\n",
      "1221/1221 [==============================] - 2s 2ms/step - loss: 0.3480 - accuracy: 0.8393 - val_loss: 0.3402 - val_accuracy: 0.8390\n",
      "Epoch 6/10\n",
      "1221/1221 [==============================] - 3s 2ms/step - loss: 0.3466 - accuracy: 0.8396 - val_loss: 0.3391 - val_accuracy: 0.8414\n",
      "Epoch 7/10\n",
      "1221/1221 [==============================] - 2s 2ms/step - loss: 0.3469 - accuracy: 0.8407 - val_loss: 0.3393 - val_accuracy: 0.8430\n",
      "Epoch 8/10\n",
      "1221/1221 [==============================] - 2s 2ms/step - loss: 0.3481 - accuracy: 0.8399 - val_loss: 0.3397 - val_accuracy: 0.8404\n",
      "Epoch 9/10\n",
      "1221/1221 [==============================] - 3s 2ms/step - loss: 0.3466 - accuracy: 0.8400 - val_loss: 0.3384 - val_accuracy: 0.8423\n",
      "Epoch 10/10\n",
      "1221/1221 [==============================] - 2s 2ms/step - loss: 0.3478 - accuracy: 0.8393 - val_loss: 0.3401 - val_accuracy: 0.8392\n"
     ]
    }
   ],
   "source": [
    "for model in [NN]:  # [NN_drop, NN]:\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=learning_rate),\n",
    "        loss=BinaryCrossentropy(from_logits=True, reduction=Reduction.SUM_OVER_BATCH_SIZE),\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    model.fit(train_ds, epochs=epochs, validation_data=test_ds, validation_freq=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
